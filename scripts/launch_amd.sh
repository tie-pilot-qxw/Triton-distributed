#!/bin/bash
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
SCRIPT_DIR=$(realpath ${SCRIPT_DIR})
DISTRIBUTED_DIR=$(dirname -- "$SCRIPT_DIR")
TRITON_ROCSHMEM_DIR=${SCRIPT_DIR}/../shmem/rocshmem_bind/python
PYROCSHMEM_DIR=${SCRIPT_DIR}/../shmem/rocshmem_bind/pyrocshmem
ROCSHMEM_ROOT=${SCRIPT_DIR}/../shmem/rocshmem_bind/rocshmem_build/install
MPI_ROOT=/opt/ompi_build/install/ompi

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${ROCSHMEM_ROOT}/lib:${MPI_ROOT}/lib

case ":${PYTHONPATH}:" in
    *:"${DISTRIBUTED_DIR}/python:${PYROCSHMEM_DIR}/build:${TRITON_ROCSHMEM_DIR}":*)
        ;;
    *)
        export PYTHONPATH="${PYTHONPATH}:${DISTRIBUTED_DIR}/python:${PYROCSHMEM_DIR}/build:${TRITON_ROCSHMEM_DIR}"
        ;;
esac

export TRITON_CACHE_DIR=triton_cache
export ROCSHMEM_HOME=${ROCSHMEM_ROOT}

## AMD env vars
export TRITON_HIP_USE_BLOCK_PINGPONG=1 # for gemm perf
export GPU_STREAMOPS_CP_WAIT=1
export DEBUG_CLR_KERNARG_HDP_FLUSH_WA=1
# export AMD_LOG_LEVEL=5 # for debug

mkdir -p triton_cache

nproc_per_node=${ARNOLD_WORKER_GPU:=$(rocm-smi | grep W | wc -l)}
nnodes=${ARNOLD_WORKER_NUM:=1}
node_rank=${ARNOLD_ID:=0}

master_addr=${ARNOLD_WORKER_0_HOST:="127.0.0.1"}
if [ -z ${ARNOLD_WORKER_0_PORT} ]; then
  master_port="23456"
else
  master_port=$(echo "$ARNOLD_WORKER_0_PORT" | cut -d "," -f 1)
fi

additional_args="--rdzv_endpoint=${master_addr}:${master_port}"

CMD="torchrun \
  --node_rank=${node_rank} \
  --nproc_per_node=${nproc_per_node} \
  --nnodes=${nnodes} \
  ${additional_args} \
  $@"

CMD="${MPI_ROOT}/bin/mpirun --allow-run-as-root -n ${nproc_per_node} -x MASTER_ADDR=127.0.0.1 -x MASTER_PORT=23456 python3 $@"

echo ${CMD}
${CMD}

ret=$?
exit $ret
